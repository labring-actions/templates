apiVersion: app.sealos.io/v1
kind: Template
metadata:
  name: laf
spec:
  title: "Laf"
  url: "https://laf.run/"
  gitRepo: "https://github.com/labring/laf"
  author: "Laf"
  description: "Laf 是开源的云开发平台，提供云函数、云数据库、云存储等开箱即用的应用资源。让开发者专注于业务开发，无需折腾服务器，快速释放创意。"
  readme: "https://raw.githubusercontent.com/labring/laf/main/README.md"
  icon: "https://laf.run/homepage/logo_icon.svg"
  templateType: inline
  locale: zh
  draft: true
  defaults:
    app_name:
      type: string
      value: laf-${{ random(8) }}
    exporter_jwt:
      type: string
      value: ${{ random(32) }}
    server_jwt:
      type: string
      value: ${{ random(32) }}
    minio_passwd:
      type: string
      value: ${{ random(24) }}
    mongo_express:
      type: string
      value: ${{ random(24) }}
  inputs:
    minio_storage:
      description: "Storage size for minio in Gi"
      type: number
      default: "5"
      required: true
    prometheus_storage:
      description: "Storage size for Prometheus in Gi"
      type: number
      default: "10"
      required: true
    domain:
      description: "the domain of laf"
      type: string
      default: laf-${{ random(8) }}
      required: true
---
# mongo sa
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    sealos-db-provider-cr: ${{ defaults.app_name }}-mongo
    app.kubernetes.io/instance: ${{ defaults.app_name }}-mongo
    app.kubernetes.io/managed-by: kbcli
  name: ${{ defaults.app_name }}-mongo

---

# laf sa
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-server
  name: ${{ defaults.app_name }}-sa

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    sealos-db-provider-cr: ${{ defaults.app_name }}-mongo
    app.kubernetes.io/instance: ${{ defaults.app_name }}-mongo
    app.kubernetes.io/managed-by: kbcli
  name: ${{ defaults.app_name }}-mongo
rules:
  - apiGroups:
      - '*'
    resources:
      - '*'
    verbs:
      - '*'

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    sealos-db-provider-cr: ${{ defaults.app_name }}-mongo
    app.kubernetes.io/instance: ${{ defaults.app_name }}-mongo
    app.kubernetes.io/managed-by: kbcli
  name: ${{ defaults.app_name }}-mongo
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ${{ defaults.app_name }}-mongo
subjects:
  - kind: ServiceAccount
    name: ${{ defaults.app_name }}-mongo

---
# laf mongodb
apiVersion: apps.kubeblocks.io/v1alpha1
kind: Cluster
metadata:
  finalizers:
    - cluster.kubeblocks.io/finalizer
  labels:
    clusterdefinition.kubeblocks.io/name: mongodb
    clusterversion.kubeblocks.io/name: mongodb-5.0
    sealos-db-provider-cr: ${{ defaults.app_name }}-mongo
  annotations: {}
  name: ${{ defaults.app_name }}-mongo
  generation: 1
spec:
  affinity:
    nodeLabels: {}
    podAntiAffinity: Preferred
    tenancy: SharedNode
    topologyKeys: []
  clusterDefinitionRef: mongodb
  clusterVersionRef: mongodb-5.0
  componentSpecs:
    - componentDefRef: mongodb
      monitor: true
      name: mongodb
      replicas: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 250m
          memory: 256Mi
      serviceAccountName: ${{ defaults.app_name }}-mongo
      volumeClaimTemplates:
        - name: data
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 5Gi

  terminationPolicy: Delete
  tolerations: []

---

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-server
  name: ${{ defaults.app_name }}-role
rules:
  - apiGroups:
      - "*"
    resources:
      - "*"
    verbs:
      - "*"

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-server
  name: ${{ defaults.app_name }}-rolebind
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ${{ defaults.app_name }}-role
subjects:
  - kind: ServiceAccount
    name: ${{ defaults.app_name }}-sa

---
# laf-server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${{ defaults.app_name }}-server
  labels:
    app: ${{ defaults.app_name }}-server
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ${{ defaults.app_name }}-server
  template:
    metadata:
      labels:
        app: ${{ defaults.app_name }}-server
    spec:
      automountServiceAccountToken: true
      serviceAccountName: ${{ defaults.app_name }}-sa
      securityContext: {}
      containers:
        - name: ${{ defaults.app_name }}-server
          securityContext: {}
          image: "docker.io/lafyun/laf-server:sha-ef30cd9"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /v1/regions
              port: http
          readinessProbe:
            httpGet:
              path: /v1/regions
              port: http
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: DISABLED_BILLING_CREATION_TASK
              value: "true"
            - name: DISABLED_BILLING_PAYMENT_TASK
              value: "true"
            - name: DISABLED_DATABASE_USAGE_CAPTURE_TASK
              value: "true"
            - name: DISABLED_DATABASE_USAGE_LIMIT_TASK
              value: "true"
            - name: DISABLED_STORAGE_USAGE_CAPTURE_TASK
              value: "true"
            - name: DISABLED_STORAGE_USAGE_LIMIT_TASK
              value: "true"
            - name: DEFAULT_RUNTIME_IMAGE
              value: docker.io/lafyun/runtime-node:sha-fbfa762
            - name: DEFAULT_RUNTIME_INIT_IMAGE
              value: docker.io/lafyun/runtime-node-init:sha-fbfa762
            - name: APPID_LENGTH
              value: 8
            - name: DEFAULT_REGION_NAMESPACE
              value: ${{ SEALOS_NAMESPACE }}
            - name: DEFAULT_REGION_TLS_ENABLED
              value: true
            - name: DEFAULT_REGION_TLS_WILDCARD_CERTIFICATE_SECRET_NAME
              value: ${{ SEALOS_CERT_SECRET_NAME }}
            - name: MONGO_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: ${{ defaults.app_name }}-mongo-conn-credential
                  key: password
            - name: DATABASE_URL
              value: "mongodb://root:$(MONGO_PASSWORD)@${{ defaults.app_name }}-mongo-mongodb.${{ SEALOS_NAMESPACE }}.svc:27017/sys_db?authSource=admin&w=majority"
            - name: JWT_SECRET
              value: ${{ defaults.server_jwt }}
            - name: API_SERVER_URL
              value: "https://api-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}"
            - name: JWT_EXPIRES_IN
              value: "7d"
            - name: DEFAULT_REGION_DATABASE_URL
              value: "mongodb://root:$(MONGO_PASSWORD)@${{ defaults.app_name }}-mongo-mongodb.${{ SEALOS_NAMESPACE }}.svc:27017/sys_db?authSource=admin&w=majority"
            - name: DEFAULT_REGION_MINIO_DOMAIN
              value: "oss-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}"
            - name: DEFAULT_REGION_MINIO_EXTERNAL_ENDPOINT
              value: "https://oss-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}"
            - name: DEFAULT_REGION_MINIO_INTERNAL_ENDPOINT
              value: "http://${{ defaults.app_name }}-minio.${{ SEALOS_NAMESPACE }}.svc.cluster.local:9000"
            - name: DEFAULT_REGION_MINIO_ROOT_ACCESS_KEY
              value: ${{ defaults.minio_passwd }}
            - name: DEFAULT_REGION_MINIO_ROOT_SECRET_KEY
              value: ${{ defaults.minio_passwd }}
            - name: DEFAULT_REGION_RUNTIME_DOMAIN
              value: "${{ SEALOS_CLOUD_DOMAIN }}"
            - name: DEFAULT_REGION_WEBSITE_DOMAIN
              value: "${{ SEALOS_CLOUD_DOMAIN }}"
            - name: DEFAULT_REGION_PROMETHEUS_URL
              value: http://prometheus-${{ defaults.app_name }}.${{ SEALOS_NAMESPACE }}.svc.cluster.local:9090
            - name: SITE_NAME
              value: "${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}"
---
apiVersion: v1
kind: Service
metadata:
  name: ${{ defaults.app_name }}-server
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-server
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: ${{ defaults.app_name }}-server
---
# laf-web
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${{ defaults.app_name }}-web
  labels:
    app: ${{ defaults.app_name }}-web
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ${{ defaults.app_name }}-web
  template:
    metadata:
      labels:
        app: ${{ defaults.app_name }}-web
    spec:
      serviceAccountName: default
      securityContext: {}
      containers:
        - name: ${{ defaults.app_name }}-web
          securityContext: {}
          image: "docker.io/lafyun/laf-web:sha-e9d012d"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 10Mi

---
apiVersion: v1
kind: Service
metadata:
  name: ${{ defaults.app_name }}-web
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-web
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: ${{ defaults.app_name }}-web

---
# runtime exporter
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-runtime-exporter
    app: ${{ defaults.app_name }}-runtime-exporter
  name: ${{ defaults.app_name }}-runtime-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ${{ defaults.app_name }}-runtime-exporter
  template:
    metadata:
      labels:
        app: ${{ defaults.app_name }}-runtime-exporter
    spec:
      automountServiceAccountToken: true
      serviceAccountName: ${{ defaults.app_name }}-sa
      securityContext: {}
      containers:
        - image: docker.io/lafyun/runtime-exporter:sha-2948df9
          imagePullPolicy: IfNotPresent
          name: ${{ defaults.app_name }}-runtime-exporter
          ports:
            - name: http
              containerPort: 2342
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
          readinessProbe:
            httpGet:
              path: /healthz
              port: http
          resources:
            limits:
              cpu: 150m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          env:
            - name: API_SECRET
              value: ${{ defaults.exporter_jwt }}
            - name: NAMESPACE
              value: ${{ SEALOS_NAMESPACE }}
            - name: DB_NAMESPACE
              value: ${{ SEALOS_NAMESPACE }}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-runtime-exporter
  name: ${{ defaults.app_name }}-runtime-exporter
spec:
  ports:
    - name: http
      port: 2342
      protocol: TCP
      targetPort: http
  selector:
    app: ${{ defaults.app_name }}-runtime-exporter
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-runtime-exporter
  name: ${{ defaults.app_name }}-runtime-exporter
spec:
  endpoints:
    - interval: 30s
      path: "/runtime/metrics/${{ defaults.exporter_jwt }}"
      scrapeTimeout: 10s
      honorLabels: true
    - interval: 30s
      path: "/database/metrics/${{ defaults.exporter_jwt }}"
      scrapeTimeout: 10s
      honorLabels: true
  namespaceSelector:
    matchNames:
      - ${{ SEALOS_NAMESPACE }}
  selector:
    matchLabels:
      cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-runtime-exporter
---
# kubeblock mongodb prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ${{ defaults.app_name }}-laf-mongodb
spec:
  endpoints:
    - interval: 30s
      path: /metrics
      port: http-metrics
  namespaceSelector:
    matchNames:
      - ${{ SEALOS_NAMESPACE }}
  selector:
    matchLabels:
      app.kubernetes.io/name: mongodb
      apps.kubeblocks.io/component-name: mongodb
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-runtime-exporter
  name: ${{ defaults.app_name }}-prometheus-laf-billing.rules
spec:
  groups:
    - name: ${{ defaults.app_name }}-prometheus-laf-billing.rules
      interval: 30s
      rules:
        - record: laf:billing:cpu
          expr: max_over_time(sum by (appid) (laf_runtime_cpu_limit{container!=""})[1h:])
        - record: laf:billing:memory
          expr: max_over_time(sum by (appid) (laf_runtime_memory_limit{container!=""})[1h:])
---
## Prometheus
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    cloud.sealos.io/app-deploy-manager: prometheus-${{ defaults.app_name }}
    app: prometheus-${{ defaults.app_name }}
  name: ${{ defaults.app_name }}
spec:
  podMetadata:
    labels:
      app: prometheus-${{ defaults.app_name }}
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 50m
      memory: 128Mi
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  evaluationInterval: 60s
  image: quay.io/prometheus/prometheus:v2.45.0
  serviceMonitorSelector: {}
  probeSelector: {}
  ruleSelector: {}
  portName: http-web
  retention: 5d
  scrapeInterval: 60s
  serviceAccountName: ${{ defaults.app_name }}-sa
  replicas: 1
  shards: 1
  storage:
    volumeClaimTemplate:
      metadata:
        annotations:
          path: /prometheus
          value: " ${{ inputs.prometheus_storage }} "
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: ${{ inputs.prometheus_storage }}Gi
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-${{ defaults.app_name }}
  labels:
    cloud.sealos.io/app-deploy-manager: prometheus-${{ defaults.app_name }}
spec:
  ports:
    - port: 9090
      targetPort: http-web
      protocol: TCP
      name: http-web
  selector:
    app: prometheus-${{ defaults.app_name }}
  type: ClusterIP
---
# minio prometheus probe
apiVersion: monitoring.coreos.com/v1
kind: Probe
metadata:
  name: ${{ defaults.app_name }}-minio
spec:
  jobName: ${{ defaults.app_name }}-minio-probe-job
  prober:
    path: /minio/v2/metrics/cluster
    scheme: http
    url: ${{ defaults.app_name }}-minio.${{ SEALOS_NAMESPACE }}:9000
  targets:
    staticConfig:
      static:
        - ${{ defaults.app_name }}-minio.${{ SEALOS_NAMESPACE }}
---
#laf minio
apiVersion: v1
kind: Secret
metadata:
  name: ${{ defaults.app_name }}-minio-secret
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio-secret
type: Opaque
stringData:
  rootUser: ${{ defaults.minio_passwd }}
  rootPassword: ${{ defaults.minio_passwd }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ${{ defaults.app_name }}-minio-conf
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio-conf
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }

    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
      VERSIONING=$4
      OBJECTLOCKING=$5

      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi

    # Create the bucket if it does not exist and set objectlocking if enabled (NOTE: versioning will be not changed if OBJECTLOCKING is set because it enables versioning to the Buckets created)
    if ! checkBucketExists $BUCKET ; then
        if [ ! -z $OBJECTLOCKING ] ; then
          if [ $OBJECTLOCKING = true ] ; then
              echo "Creating bucket with OBJECTLOCKING '$BUCKET'"
              ${MC} mb --with-lock myminio/$BUCKET
          elif [ $OBJECTLOCKING = false ] ; then
                echo "Creating bucket '$BUCKET'"
                ${MC} mb myminio/$BUCKET
          fi
      elif [ -z $OBJECTLOCKING ] ; then
            echo "Creating bucket '$BUCKET'"
            ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."
      fi
      fi


      # set versioning for bucket if objectlocking is disabled or not set
      if [ -z $OBJECTLOCKING ] ; then
      if [ ! -z $VERSIONING ] ; then
        if [ $VERSIONING = true ] ; then
            echo "Enabling versioning for '$BUCKET'"
            ${MC} version enable myminio/$BUCKET
        elif [ $VERSIONING = false ] ; then
            echo "Suspending versioning for '$BUCKET'"
            ${MC} version suspend myminio/$BUCKET
        fi
        fi
      else
          echo "Bucket '$BUCKET' versioning unchanged."
      fi


      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} anonymous set $POLICY myminio/$BUCKET
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme

  add-user: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_tmp"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # checkUserExists ()
    # Check if the user exists, by using the exit code of `mc admin user info`
    checkUserExists() {
      CMD=$(${MC} admin user info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }

    # createUser ($policy)
    createUser() {
      POLICY=$1
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      USER=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the user if it does not exist
      if ! checkUserExists ; then
        echo "Creating user '$USER'"
        cat $MINIO_ACCESSKEY_SECRETKEY_TMP | ${MC} admin user add myminio
      else
        echo "User '$USER' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP

      # set policy for user
      if [ ! -z $POLICY -a $POLICY != " " ] ; then
          echo "Adding policy '$POLICY' for '$USER'"
          ${MC} admin policy set myminio $POLICY user=$USER
      else
          echo "User '$USER' has no policy attached."
      fi
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme

  add-policy: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # checkPolicyExists ($policy)
    # Check if the policy exists, by using the exit code of `mc admin policy info`
    checkPolicyExists() {
      POLICY=$1
      CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)
      return $?
    }

    # createPolicy($name, $filename)
    createPolicy () {
      NAME=$1
      FILENAME=$2

      # Create the name if it does not exist
      echo "Checking policy: $NAME (in /config/$FILENAME.json)"
      if ! checkPolicyExists $NAME ; then
        echo "Creating policy '$NAME'"
      else
        echo "Policy '$NAME' already exists."
      fi
      ${MC} admin policy add myminio $NAME /config/$FILENAME.json

    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme


    # Create the policies
    createPolicy laf_owner_by_prefix policy_0
    createPolicy laf_owner_readonly_by_prefix policy_1
  # laf_owner_by_prefix
  policy_0.json: |-
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Action": [
    "s3:GetBucketPolicy",
    "s3:GetObject",
    "s3:ListBucket",
    "s3:ListBucketMultipartUploads",
    "s3:ListMultipartUploadParts",
    "s3:PutObject",
    "s3:DeleteObject",
    "s3:GetBucketLocation"
          ],
          "Resource": [
    "arn:aws:s3:::${aws:username}-*"
          ]
        }
      ]
    }

  # laf_owner_readonly_by_prefix
  policy_1.json: |-
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Action": [
    "s3:GetBucketPolicy",
    "s3:GetObject",
    "s3:ListBucket",
    "s3:ListBucketMultipartUploads",
    "s3:ListMultipartUploadParts",
    "s3:DeleteObject",
    "s3:GetBucketLocation"
          ],
          "Resource": [
    "arn:aws:s3:::${aws:username}-*"
          ]
        }
      ]
    }

  custom-command: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # runCommand ($@)
    # Run custom mc command
    runCommand() {
      ${MC} "$@"
      return $?
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme


    # Run custom commands
    runCommand alias ls && ls /config
    runCommand admin user add myminio temp-user abcd123456
    runCommand admin group add myminio laf_owner_by_prefix_group temp-user
    runCommand admin group add myminio laf_owner_readonly_by_prefix_group temp-user
    runCommand admin policy add myminio laf_owner_by_prefix /config/policy_0.json
    runCommand admin policy set myminio laf_owner_by_prefix group=laf_owner_by_prefix_group
    runCommand admin policy add myminio laf_owner_readonly_by_prefix /config/policy_1.json
    runCommand admin policy set myminio laf_owner_readonly_by_prefix group=laf_owner_readonly_by_prefix_group
---
apiVersion: v1
kind: Service
metadata:
  name: ${{ defaults.app_name }}-minio
  labels:
    app: ${{ defaults.app_name }}-minio
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio
spec:
  type: ClusterIP
  ports:
    - name: api
      port: 9000
      protocol: TCP
      targetPort: 9000
    - name: console
      port: 9001
      protocol: TCP
      targetPort: 9001
  selector:
    app: ${{ defaults.app_name }}-minio
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ${{ defaults.app_name }}-minio
  annotations:
    originImageName: quay.io/minio/minio
    deploy.cloud.sealos.io/minReplicas: "1"
    deploy.cloud.sealos.io/maxReplicas: "1"
  labels:
    app: ${{ defaults.app_name }}-minio
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio
spec:
  replicas: 1
  revisionHistoryLimit: 1
  minReadySeconds: 10
  serviceName: ${{ defaults.app_name }}-minio-svc
  selector:
    matchLabels:
      app: ${{ defaults.app_name }}-minio
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: ${{ defaults.app_name }}-minio
    spec:
      terminationGracePeriodSeconds: 10
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: ${{ defaults.app_name }}-minio
          image: quay.io/minio/minio:RELEASE.2023-03-22T06-36-24Z
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          command:
            [
              "/bin/sh",
              "-ce",
              "/usr/bin/docker-entrypoint.sh minio server  /data -S /etc/minio/certs/ --address :9000 --console-address :9001",
            ]
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: ${{ defaults.app_name }}-minio-secret
                  key: rootUser
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: ${{ defaults.app_name }}-minio-secret
                  key: rootPassword
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: public
          ports:
            - name: http
              containerPort: 9000
            - name: http-console
              containerPort: 9001
          resources:
            limits:
              cpu: 300m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          volumeMounts:
            - name: data
              mountPath: /data
      volumes: []
  volumeClaimTemplates:
    - metadata:
        annotations:
          path: /data
          value: " ${{ inputs.minio_storage }} "
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: ${{ inputs.minio_storage }}Gi

---
# Source: minio/templates/post-install-create-policy-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ${{ defaults.app_name }}-minio-make-policies-job
  labels:
    app: ${{ defaults.app_name }}-minio-make-policies-job
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio-make-policies-job
spec:
  ttlSecondsAfterFinished: 100
  template:
    metadata:
      labels:
        app: ${{ defaults.app_name }}-minio-make-policies-job
    spec:
      securityContext:
        runAsNonRoot: false
        seccompProfile:
          type: RuntimeDefault
      restartPolicy: OnFailure
      volumes:
        - name: minio-configuration
          projected:
            sources:
              - configMap:
                  name: ${{ defaults.app_name }}-minio-conf
              - secret:
                  name: ${{ defaults.app_name }}-minio-secret
      containers:
        - name: ${{ defaults.app_name }}-minio-make-policies-job
          image: "quay.io/minio/mc:RELEASE.2022-11-07T23-47-39Z"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          command: ["/bin/sh", "/config/add-policy"]
          env:
            - name: MINIO_ENDPOINT
              value: "${{ defaults.app_name }}-minio.${{ SEALOS_NAMESPACE }}.svc.cluster.local"
            - name: MINIO_PORT
              value: "9000"
          volumeMounts:
            - name: minio-configuration
              mountPath: /config
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
---
# Source: minio/templates/post-install-custom-command.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ${{ defaults.app_name }}-minio-custom-command-job
  labels:
    app: ${{ defaults.app_name }}-minio-custom-command-job
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio-custom-command-job
spec:
  ttlSecondsAfterFinished: 100
  template:
    metadata:
      labels:
        app: ${{ defaults.app_name }}-minio-custom-command-job
    spec:
      securityContext:
        runAsNonRoot: false
        seccompProfile:
          type: RuntimeDefault
      restartPolicy: OnFailure
      volumes:
        - name: minio-configuration
          projected:
            sources:
              - configMap:
                  name: ${{ defaults.app_name }}-minio-conf
              - secret:
                  name: ${{ defaults.app_name }}-minio-secret
      containers:
        - name: ${{ defaults.app_name }}-minio-custom-command-job
          image: "quay.io/minio/mc:RELEASE.2022-11-07T23-47-39Z"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          command: ["/bin/sh", "/config/custom-command"]
          env:
            - name: MINIO_ENDPOINT
              value: "${{ defaults.app_name }}-minio.${{ SEALOS_NAMESPACE }}.svc.cluster.local"
            - name: MINIO_PORT
              value: "9000"
          volumeMounts:
            - name: minio-configuration
              mountPath: /config
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
---
# ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ${{ defaults.app_name }}-web
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-web
    cloud.sealos.io/app-deploy-manager-domain: ${{ inputs.domain }}
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: 32m
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_buffer_size 64k;
      large_client_header_buffers 4 128k;
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/client-body-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($request_uri ~* \.(js|css|gif|jpe?g|png)) {
        expires 30d;
        add_header Cache-Control "public";
      }
spec:
  rules:
    - host: ${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      http:
        paths:
          - backend:
              service:
                name: ${{ defaults.app_name }}-web
                port:
                  number: 80
            path: /
            pathType: Prefix
          - backend:
              service:
                name: ${{ defaults.app_name }}-server
                port:
                  number: 3000
            path: /v1/
            pathType: Prefix
  tls:
    - hosts:
        - ${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      secretName: ${{ SEALOS_CERT_SECRET_NAME }}
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ${{ defaults.app_name }}-server
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-server
    cloud.sealos.io/app-deploy-manager-domain: api-${{ inputs.domain }}
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: 32m
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_buffer_size 64k;
      large_client_header_buffers 4 128k;
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/client-body-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($request_uri ~* \.(js|css|gif|jpe?g|png)) {
        expires 30d;
        add_header Cache-Control "public";
      }
spec:
  rules:
    - host: api-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      http:
        paths:
          - backend:
              service:
                name: ${{ defaults.app_name }}-server
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - api-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      secretName: ${{ SEALOS_CERT_SECRET_NAME }}

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ${{ defaults.app_name }}-minio-console
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio
    cloud.sealos.io/app-deploy-manager-domain: minio-${{ inputs.domain }}
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: 32m
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_buffer_size 64k;
      large_client_header_buffers 4 128k;
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/client-body-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($request_uri ~* \.(js|css|gif|jpe?g|png)) {
        expires 30d;
        add_header Cache-Control "public";
      }
spec:
  rules:
    - host: minio-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      http:
        paths:
          - pathType: Prefix
            path: /
            backend:
              service:
                name: ${{ defaults.app_name }}-minio
                port:
                  number: 9001
  tls:
    - hosts:
        - minio-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      secretName: ${{ SEALOS_CERT_SECRET_NAME }}

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ${{ defaults.app_name }}-minio-api
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-minio
    cloud.sealos.io/app-deploy-manager-domain: oss-${{ inputs.domain }}
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: 32m
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_buffer_size 64k;
      large_client_header_buffers 4 128k;
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/client-body-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($request_uri ~* \.(js|css|gif|jpe?g|png)) {
        expires 30d;
        add_header Cache-Control "public";
      }
spec:
  rules:
    - host: oss-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      http:
        paths:
          - pathType: Prefix
            path: /
            backend:
              service:
                name: ${{ defaults.app_name }}-minio
                port:
                  number: 9000
  tls:
    - hosts:
        - oss-${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
      secretName: ${{ SEALOS_CERT_SECRET_NAME }}
---
# cert issuer
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: laf-issuer
  labels:
    cloud.sealos.io/app-deploy-manager: laf-issuer
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: ${{ defaults.app_name }}@${{ inputs.domain }}.io
    privateKeySecretRef:
      name: laf-letsencrypt-prod
    solvers:
      - http01:
          ingress:
            class: nginx
            serviceType: ClusterIP
---
# shortcut
apiVersion: app.sealos.io/v1
kind: App
metadata:
  name: ${{ defaults.app_name }}
  labels:
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}
spec:
  data:
    url: https://${{ inputs.domain }}.${{ SEALOS_CLOUD_DOMAIN }}
  displayType: normal
  icon: "https://laf.run/homepage/logo_icon.svg"
  name: ${{ defaults.app_name }}
  type: iframe

---
# Template usage statistics
apiVersion: batch/v1
kind: Job
metadata:
  name: ${{ defaults.app_name }}-statistics
  labels:
    app: ${{ defaults.app_name }}-statistics
    cloud.sealos.io/app-deploy-manager: ${{ defaults.app_name }}-statistics
spec:
  ttlSecondsAfterFinished: 100
  template:
    metadata:
      labels:
        app: ${{ defaults.app_name }}-statistics
    spec:
      securityContext:
        runAsNonRoot: false
        seccompProfile:
          type: RuntimeDefault
      restartPolicy: OnFailure
      volumes: []
      initContainers:
        - name: ${{ defaults.app_name }}-runtime-node
          image: "docker.io/lafyun/runtime-node:sha-67b3cd6"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          command:
            - sh
            - "-c"
          args:
            - >-
              sleep 1
          volumeMounts: []
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
        - name: ${{ defaults.app_name }}-runtime-node-init
          image: "docker.io/lafyun/runtime-node-init:sha-67b3cd6"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          command:
            - sh
            - "-c"
          args:
            - >-
              sleep 1
          volumeMounts: []
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
      containers:
        - name: ${{ defaults.app_name }}-statistics
          image: "docker.io/library/nginx:latest"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          command:
            - sh
            - "-c"
          args:
            - >-
              curl -X POST -H "Content-Type: application/json" -d '{"namespace":
              "${{ SEALOS_NAMESPACE }}", "sealos-domain": "${{ SEALOS_CLOUD_DOMAIN }}", "laf-server":"${{ defaults.app_name }}-server","laf-domain":"${{inputs.domain}}"}'
              https://t0qxj6.laf.run/template-statistics
          volumeMounts: []
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi